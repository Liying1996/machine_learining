{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SVM的概念\n",
    "\n",
    "支撑向量机（support vector machine）既可以做回归，也可以做分类。SVM尝试寻找到一个最优的决策边界，距离两个类别的最近的样本最远。如图所示，最近的决定的样本点就是支撑向量。SVM要做的事情就是最大化图中的margin。\n",
    "\n",
    "![](https://www.saedsayad.com/images/SVM_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知点(x,y)到直线Ax + by + c = 0的距离为：\n",
    "\n",
    "\\begin{equation}\n",
    "d = \\frac{|Ax + By + C|}{\\sqrt{A^2+B^2}}\n",
    "\\end{equation}\n",
    "\n",
    "如果将线性函数放到n维向量中为：\n",
    "\n",
    "\\begin{equation}\n",
    "w^Tx + b = 0\n",
    "\\end{equation}\n",
    "\n",
    "可求得点(x,y)到该直线的距离为：\n",
    "\n",
    "\\begin{equation}\n",
    "d = \\frac{|w^Tx + b|}{\\|w\\|}\n",
    "\\end{equation}\n",
    "\n",
    "这里 $\\|w\\|$为：\n",
    "\n",
    "\\begin{equation}\n",
    "\\|w\\| = \\sqrt{w_1^2 + w_2^2 + w_3^2 + ... + w_n^2}\n",
    "\\end{equation}\n",
    "\n",
    "假如距离决策边界最近的点到决策边界的距离为d, 那么对于决策边界上下其他的点，我们可以知道某一点到决策边界的距离是大于等于d的，即（假如决策边界上面的为1，下面的为-1）：\n",
    "\n",
    "![](https://www.saedsayad.com/images/SVM_optimize.png)\n",
    "\n",
    "公式为：\n",
    "\n",
    "\\begin{equation}\n",
    "y=\\left\\{\n",
    "             \\begin{array}{svm}\n",
    "             \\frac{|w^Tx\\left(i\\right) + b|}{\\|w\\|} \\ge d, & \\forall y = 1 \\\\\n",
    "             \\\\\n",
    "             \\frac{|w^Tx\\left(i\\right) + b|}{\\|w\\|} \\le d , & \\forall y = -1 \\\\ \n",
    "             \\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "我们进一步可得到：\n",
    "\n",
    "\\begin{equation}\n",
    "y=\\left\\{\n",
    "             \\begin{array}{svm}\n",
    "             \\frac{|w^Tx^\\left(i\\right)  + b|}{\\|w\\|d} \\ge 1, & \\forall y = 1 \\\\\n",
    "             \\\\\n",
    "             \\frac{|w^Tx^\\left(i\\right)  + b|}{\\|w\\|d} \\le -1 , & \\forall y = -1 \\\\ \n",
    "             \\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "而式中的分母是个确定的数字，可得：\n",
    "\n",
    "\\begin{equation}\n",
    "y=\\left\\{\n",
    "             \\begin{array}{svm}\n",
    "             w_d^Tx^\\left(i\\right)  + b_d \\ge 1, & \\forall y = 1 \\\\\n",
    "             w_d^Tx^\\left(i\\right) + b_d \\le 1, & \\forall y = -1 \\\\ \n",
    "             \\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "为了方便书写，可以直接写为（不过此处的$w$与开始时候的已经不同了）：\n",
    "\n",
    "\\begin{equation}\n",
    "y=\\left\\{\n",
    "             \\begin{array}{svm}\n",
    "             w^Tx^\\left(i\\right)  + b \\ge 1, & \\forall y = 1 \\\\\n",
    "             \\\\\n",
    "             w^Tx^\\left(i\\right) + b \\le 1, & \\forall y = -1 \\\\ \n",
    "             \\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "这两次式子同样可以变为一个：\n",
    "\n",
    "\\begin{equation}\n",
    "y^\\left(i\\right)\\left(w^Tx^\\left(i\\right) + b \\right) \\ge 1\n",
    "\\end{equation}\n",
    "\n",
    "对于任意支撑向量，我们需要最大化：\n",
    "\\begin{equation}\n",
    "max\\frac{|w^T+b|}{\\|w\\|}\n",
    "\\end{equation}\n",
    "\n",
    "分子不是在边界上的一类就是另一类，即1或者-1，那么上式即可变为最大化：\n",
    "\n",
    "\\begin{equation}\n",
    "max\\frac{1}{\\|w\\|}\n",
    "\\end{equation}\n",
    "\n",
    "即：\n",
    "\n",
    "\\begin{equation}\n",
    "min\\|w\\|\n",
    "\\end{equation}\n",
    "\n",
    "为了方便求导，我们一般是最小化平方：\n",
    "\n",
    "\\begin{equation}\n",
    "min\\frac{1}{2}\\|w\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "我们整体即可得到 (s.t. such that 在……条件下)：\n",
    "\n",
    "\\begin{equation}\n",
    "min\\frac{1}{2}\\|w\\|^2  \\\\\n",
    "\\\\\n",
    "s.t. y^\\left(i\\right)\\left(w^Tx^\\left(i\\right) + b \\right) \\ge 1\n",
    "\\end{equation}\n",
    "\n",
    "这是一个有条件的最优化问题，难度增大了许多，需要用拉普拉斯乘子法求解（待更新）。\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Soft Margin SVM\n",
    "\n",
    "上面提到的方法是Hard Margin SVM。但是存在一种这样的情况：\n",
    "![](https://miro.medium.com/max/700/1*Z9HQRL4rTdubV2a0hLtCjA.png)\n",
    "\n",
    "红色和绿色的线哪一个更好呢？有一个绿色的方块离红色方块很近，如果按照上面的hard margin的方法很有可能找到的决策边界是红色的线。但是这样的决策边界，泛化能力可能存在问题。大多数绿色的点离红点是比较远的，而因为一个点，对决策边界造成了很大的影响，这么的点很可能是一个离群点甚至是错误的点，并不能代表一般情况。而绿色的决策边界线，虽然将其中一个点进行了错误的分类，但是在真实情况下预测的时候，可能会更好，这样的决策边界泛化能力会更好。\n",
    "\n",
    "或者更极端的情况下，绿色的点在红色点中间，这样的话，用上面的方法根本就不可分了。所以需要一个存在一定容错的SVM。\n",
    "\n",
    "因此我们对上面的条件进行限定，让我们的模型对训练集中的极端数据不那么敏感(容忍一定的错误，但是这个错误一定要最小)：\n",
    "\n",
    "![](https://miro.medium.com/max/700/1*M_3iYollNTlz0PVn5udCBQ.png)\n",
    "\n",
    "\\begin{equation}\n",
    "min\\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{m}\\zeta_i \\\\\n",
    "\\\\\n",
    "s.t. y^\\left(i\\right)\\left(w^Tx^\\left(i\\right) + b \\right) \\ge 1 - \\zeta_i \\\\\n",
    "\\zeta \\ge 0\n",
    "\\end{equation}\n",
    "\n",
    "上式即为L1正则。以下是L2正则：\n",
    "\n",
    "\\begin{equation}\n",
    "min\\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^{m}\\zeta_i^2 \\\\\n",
    "\\\\\n",
    "s.t. y^\\left(i\\right)\\left(w^Tx^\\left(i\\right) + b \\right) \\ge 1 - \\zeta_i \\\\\n",
    "\\zeta \\ge 0\n",
    "\\end{equation}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
